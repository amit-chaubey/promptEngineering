{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amitchoubey/Documents/Projects/promptEngineering/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by NASA, which succeeded in preparing and landing the first humans on the Moon from 1968 to 1972.\"\n",
    "question = \"Which organization was responsible for the Apollo program?\"\n",
    "\n",
    "input_text = f\"\"\"Answer the following question based on the given context.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=100, num_beams=4, early_stopping=True)\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)  # Expected: NASA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-Shot learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"context\": \"The Great Pyramid of Giza is the oldest of the Seven Wonders of the Ancient World, and the only one to remain largely intact.\",\n",
    "        \"question\": \"How many Wonders of the Ancient World are there?\",\n",
    "        \"answer\": \"Seven\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Photosynthesis is a process used by plants to convert light energy into chemical energy that can later be released to fuel the organisms' activities.\",\n",
    "        \"question\": \"What do plants convert light energy into?\",\n",
    "        \"answer\": \"chemical energy\"\n",
    "    }\n",
    "]\n",
    "\n",
    "current_context = \"The human brain contains approximately 86 billion neurons, which communicate via synaptic connections.\"\n",
    "current_question = \"How many neurons are in the human brain?\"\n",
    "\n",
    "input_text = \"Answer questions based on the given contexts.\\n\\n\"\n",
    "for example in examples:\n",
    "    input_text += f\"Context: {example['context']}\\n\"\n",
    "    input_text += f\"Question: {example['question']}\\n\"\n",
    "    input_text += f\"Answer: {example['answer']}\\n\\n\"\n",
    "\n",
    "input_text += f\"Context: {current_context}\\n\"\n",
    "input_text += f\"Question: {current_question}\\n\"\n",
    "input_text += \"Answer:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 billion\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)  # Expected: 86 billion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"The Treaty of Versailles was signed in 1919, exactly five years after the assassination of Archduke Franz Ferdinand which occurred in 1914.\"\n",
    "question = \"In what year was Archduke Franz Ferdinand assassinated?\"\n",
    "\n",
    "input_text = f\"\"\"Read the context carefully and think step by step to answer the question.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Let's think step by step:\n",
    "1. The Treaty of Versailles was signed in 1919\n",
    "2. It was signed exactly five years after the assassination\n",
    "3. Therefore, the assassination must have occurred five years before 1919\n",
    "4. 1919 minus 5 years is 1914\n",
    "\n",
    "The answer is:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The Treaty of Versailles was signed in 1919 2. It was signed exactly five years after the assassination 3. Therefore, the assassination must have occurred five years before 1919 4. 1919 minus 5 years is 1914\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)  # Expected: 1914"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Perseverance landed on February 18, 2021 2. Ingenuity's first flight was on April 19, 2021 3. February has 28 days in 2021 4. Days remaining in February after landing: 28 - 18 = 10 days 5. Days in March: 31 days 6. Days in April until flight: 19 days 7. Total days = 10 (Feb) + 31 (Mar) + 19 (Apr) = 60 days\n"
     ]
    }
   ],
   "source": [
    "complex_context = \"\"\"The Mars Rover Perseverance landed on February 18, 2021. Its mission is to search for signs of ancient life and collect rock and soil samples for possible return to Earth. Perseverance carries the Ingenuity helicopter, which became the first aircraft to achieve powered flight on another planet on April 19, 2021.\"\"\"\n",
    "complex_question = \"How many days after Perseverance's landing did Ingenuity achieve its first flight?\"\n",
    "\n",
    "# Using chain-of-thought prompting\n",
    "input_text = f\"\"\"Analyze the dates carefully and calculate the difference step by step.\n",
    "\n",
    "Context: {complex_context}\n",
    "\n",
    "Question: {complex_question}\n",
    "\n",
    "Let's think step by step:\n",
    "1. Perseverance landed on February 18, 2021\n",
    "2. Ingenuity's first flight was on April 19, 2021\n",
    "3. February has 28 days in 2021 (not a leap year)\n",
    "4. Days remaining in February after landing: 28 - 18 = 10 days\n",
    "5. Days in March: 31 days\n",
    "6. Days in April until flight: 19 days\n",
    "7. Total days = 10 (Feb) + 31 (Mar) + 19 (Apr) = 60 days\n",
    "\n",
    "The answer is:\"\"\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=300)\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)  # Expected: 60 days"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
